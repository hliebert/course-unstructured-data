Econometrica, Vol. 76, No. 6 (November, 2008), 1537­1557
NOTES AND COMMENTS
ON THE FAILURE OF THE BOOTSTRAP FOR MATCHING ESTIMATORS
BY ALBERTO ABADIE AND GUIDO W. IMBENS 1
Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romano (1994).
KEYWORDS: Average treatment effects, bootstrap, matching.

1. INTRODUCTION
MATCHING METHODS have become very popular for the estimation of treatment effects in the absence of experimental data.2 Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, bootstrap inference for matching estimators has not been formally justified.
This article addresses the question of the validity of the standard bootstrap for nearest-neighbor matching estimators with replacement and a fixed number of neighbors. We focus on the case of a fixed number of neighbors because it conforms to the usual practice in empirical economics, where researchers applying matching estimators typically employ nearest-neighbor matching with a very limited number of neighbors (e.g., one). We show in a simple case, with a single continuous covariate, that the standard bootstrap fails to provide asymptotically valid standard errors, in spite of the fact that the matching estimator is root-N consistent and asymptotically normal with no asymptotic bias. We show that the average bootstrap variance can overestimate as well as underestimate the asymptotic variance of matching estimators. We provide some intuition for the failure of the bootstrap in this context.3

1We are grateful for comments by Peter Bickel, Stéphane Bonhomme, Joel Horowitz, Francis Kramarz, Whitney Newey, seminar participants at Princeton, CEMFI, CREST, and Harvard/MIT, and two anonymous reviewers. Financial support for this research was generously provided through NSF Grants SES-0350645 (Abadie), SES-0136789, and SES-0452590 (Imbens).
2For example, Dehejia and Wahba (1999). See Rosenbaum (2001) and Imbens (2004) for surveys.
3Other examples of failure of the bootstrap arise in the contexts of estimating the maximum of the support of a random variable (Bickel and Freedman (1981)), estimating the average of a

© 2008 The Econometric Society

DOI: 10.3982/ECTA6474

1538

A. ABADIE AND G. W. IMBENS

There are valid alternatives to the bootstrap for inference with matching estimators. In Abadie and Imbens (2006a), we derived conditions under which the nearest-neighbor matching estimator with replacement and a fixed number of matches is root-N consistent and asymptotically normal, and we proposed an analytic estimator of the asymptotic variance. Under those conditions, the validity of certain alternatives to the bootstrap, such as subsampling (Politis and Romano (1994)) or the M-out-of-N bootstrap (Bickel, Götze, and van Zwet (1997)), can be established from general results.4

2. SETUP
2.1. Basic Model
In this article we adopt the standard model of treatment effects under unconfoundedness (Rosenbaum and Rubin (1983), Heckman, Ichimura, and Todd (1998), Rosenbaum (2001), Imbens (2004)). The goal is to evaluate the effect of a treatment on the basis of data on outcomes, treatments, and covariates for treated and untreated units. We have a random sample of N0 units from the control (untreated) population and a random sample of N1 units from the treated population, with total sample size N = N0 + N1. Each unit is characterized by a pair of potential outcomes, Yi(0) and Yi(1), denoting the outcomes under the control and active treatments, respectively. We observe Yi(0) for units in the control sample and Yi(1) for units in the treated sample. For all units, we observe a covariate vector, Xi.5 Let Wi indicate whether a unit is from the control sample (Wi = 0) or the treatment sample (Wi = 1). For each unit, we observe the triple (Xi Wi Yi), where Yi = WiYi(1) + (1 - Wi)Yi(0) is the observed outcome. Let X be an N-column matrix with column i equal to Xi and assume analogous notation for Y and W.
In this article, we focus on matching estimation of the average treatment effect for the treated:
 = E[Yi(1) - Yi(0)|Wi = 1]
We make the following two identifying assumptions:

variable with infinite variance (Arthreya (1987)), and superefficient estimation (Beran (1984)). Resampling inference in these contexts can be conducted using alternative methods such as subsampling (Politis and Romano (1994)) and versions of the bootstrap where the size of the bootstrap sample is smaller than the sample size (e.g., Bickel, Götze, and van Zwet (1997)). See Hall (1992) and Horowitz (2003) for general discussions.
4See, for example, Politis, Romano, and Wolf (1999). 5To simplify our proof of lack of validity of the bootstrap, we will consider in our calculations the case of a scalar covariate. With higher dimensional covariates there is the additional complication of biases that may dominate the asymptotic distribution of matching estimators (Abadie and Imbens (2006a)).

BOOTSTRAP FOR MATCHING ESTIMATORS

1539

ASSUMPTION 2.1--Unconfoundedness: For almost all x, (Yi(1) Yi(0)) is independent of Wi conditional on Xi = x or

(Yi(0) Yi(1))  Wi| Xi = x (a.s.).

ASSUMPTION 2.2--Overlap: For some c > 0 and almost all x, c  Pr(Wi = 1|Xi = x)  1 - c

A nearest-neighbor matching estimator of  matches each treated unit i to the control unit j with the closest value for the covariate and then averages the within-pair outcome differences, Yi - Yj, over the N1 matched pairs. In this article, we focus on the case of matching with replacement, so each control unit may be used as a match for more than one treated unit.
For each treated unit i, let Di be the distance between the covariate value for observation i and the covariate value for the closest untreated match:

Di = min Xi - Xj j=1 N:Wj =0

Then let

J (i) = {j  {1 2 N} : Wj = 0 Xi - Xj = Di}

be the set of closest matches for treated unit i. If unit i is an untreated unit, then J (i) is defined to be the empty set. When Xi is continuously distributed, the set J (i) will consist of a single index with probability 1, but for bootstrap samples there will often be more than one index in this set (because an observation from the original sample may appear multiple times in the bootstrap sample). For each treated unit, i, let

Y^i(0)

=

1 #J (i)

Yj

jJ (i)

be the average outcome in the set of the closest matches for observation i, where #J (i) is the number of elements of the set J (i). The matching estimator of  is then

(1)

^

=

1 N1

(Yi
i:Wi =1

-

Y^i(0))

1540

A. ABADIE AND G. W. IMBENS

For the subsequent discussion, it is useful to write the estimator in a different

way. Let Ki denote the weighted number of times unit i is used as a match (if unit i is an untreated unit, with Ki = 0 if unit i is a treated unit):



 0

Ki = 

1{i



J

(j)}

1 #J (j)

Wj =1

if Wi = 1, if Wi = 0.

Then we can write

1N

(2)

^ =

N1

(Wi - (1 - Wi)Ki)Yi
i=1

Let

 

0

Ksq i = 

1{i  J (j)}

1

2

#J (j)

Wj =1

if Wi = 1, if Wi = 0.

In Abadie and Imbens (2006a) we proved that under certain conditions (for
example, when Xi is a scalar variable) the nearest-neighbor matching estimator in (1) is root-N consistent and asymptotically normal with zero asymptotic bias.6 We also proposed a consistent estimator for the asymptotic variance of ^:

V

AI

=

1 N12

N i=1

(Yi

-

Y^i(0)

-

^ )2

+

1 N12

N
(Ki2 - Ksq i) 2(Xi
i=1

Wi)

where 2(Xi Wi) is an estimator of the conditional variance of Yi given Wi and Xi, based on matching. Let l(i), be the closest match to unit i, in terms of the covariates, among the units with the same value for the treatment (that is, units
in the treatment groups are matched to units in the treatment group, and units in the control group are matched to units in the control group).7 Then

 2(Xi

Wi)

=

1 2 (Yi

-

Yl(i))2

6More generally, in Abadie and Imbens (2007), we proposed a bias correction that makes matching estimators root-N consistent and asymptotically normal regardless of the dimension of Xi .
7To simplify the notation, here we consider only the case without matching ties. The extension to accommodate ties is immediate, but it is not required for the purpose of the analysis in this article.

BOOTSTRAP FOR MATCHING ESTIMATORS

1541

Let V(^) be the variance of ^. In Abadie and Imbens (2006a), we showed that (under regularity conditions) the normalized version of the variance estimator, N1V AI is consistent for the normalized variance, N1V(^):
N1(V(^) - V AI) - p 0

2.2. The Bootstrap
In this article we consider two versions of the bootstrap variance commonly used in empirical research. The first version centers the bootstrap variance at the matching estimate in the original sample. The second version centers the bootstrap variance at the mean of the bootstrap distribution of the matching estimator.
Consider a random sample Z = (X W Y) with N0 controls and N1 treated units. The matching estimator, ^, is a functional t(·) of the original sample: ^ = t(Z). We construct a bootstrap sample, Zb, with N0 controls and N1 treated by sampling with replacement from the two subsamples. We then calculate the bootstrap estimator, ^b, applying the functional t(·) to the bootstrap sample: ^b = t(Zb). We denote expectations over the bootstrap distribution (conditional on the sample) as E[·|Z]. The first version of the bootstrap variance is the second moment of (^b - ^) conditional on Z:
V B I = E[(^b - ^)2|Z]

The second version of the bootstrap variance centers the bootstrap variance at the bootstrap mean, E[^b|Z], rather than at the original estimate, ^:
V B II = E (^b - E[^b|Z])2|Z

Although these bootstrap variances are defined in terms of the original
sample Z, in practice an easier way to calculate them is by drawing B bootstrap samples. Given B bootstrap samples with bootstrap estimates ^b, for b = 1 B, we can obtain unbiased estimators for these two variances as

V^ B I = 1 B

B
(^b - ^)2

and

b=1

V^ B II = 1

B

B-1

^b -

1 B

B

^b

2

b=1

b=1

We will focus on the first bootstrap variance, V B I, and its expectation, E[V B I]. We shall show that, in general, N1(E[V B I] - V(^)) does not converge to zero. We will show that in some cases the limit of N1(E[V B I] - V(^)) is
positive and that in other cases this limit is negative. As a result, we will show

1542

A. ABADIE AND G. W. IMBENS

that N1V B I is not a consistent estimator of the limit of N1V(^). This will indirectly imply that N1V B II is not consistent either. Because E[(^b - ^)2|Z]  E[(^b - E[^b|Z])2|Z], it follows that E[V B I]  E[V B II]. Thus in the cases where the limit of N1(E[V B I] - V(^)) is smaller than zero, it follows that the limit of N1(E[V B II] - V(^)) is also smaller than zero.

3. AN EXAMPLE WHERE THE BOOTSTRAP FAILS
In this section we discuss in detail a specific example where we can calculate the limits of N1V(^) and N1E[V B I].

3.1. Data Generating Process We consider the following data generating process (DGP):
ASSUMPTION 3.1: The marginal distribution of the covariate X is uniform on the interval [0 1].
ASSUMPTION 3.2: The ratio of treated and control units is N1/N0 =  for some  > 0.
ASSUMPTION 3.3: The propensity score, e(x) = Pr(Wi = 1|Xi = x), is constant.
ASSUMPTION 3.4: The distribution of Yi(1) is degenerate with Pr(Yi(1) = ) = 1, and the conditional distribution of Yi(0) given Xi = x is normal with mean 0 and variance 1.
It follows from Assumptions 3.2 and 3.3 that the propensity score is e(x) = /(1 + ).

3.2. Exact Variance and Large Sample Distribution
The data generating process implies that, conditional on X = x, the average treatment effect is equal to E[Yi(1) - Yi(0)|Xi = x] =  for all x. Therefore, the average treatment effect for the treated is equal to . Under this data generating process i WiYi/N1 = i WiYi(1)/N1 = , which along with equation (2) implies

1N

^ =  - N1

KiYi
i=1

Conditional on X and W, the only stochastic component of ^ is Y. By Assumption 3.4, given Wi = 0, the Yi's are mean zero, unit variant, and independent

BOOTSTRAP FOR MATCHING ESTIMATORS

1543

of X. Thus E[^|X W] = . Because (i) E[YiYj|Wi = 0 X W] = 0 for i = j, (ii) E[Yi2|Wi = 0 X W] = 1, and (iii) Ki is a deterministic function of X and W, it also follows that the conditional variance of ^ given X and W is

V(^|X

1 W) = N12

N
Ki2
i=1

The variance of the matching estimator is equal to the variance of E[^|X W] plus the expectation of V(^|X W). Because V(E[^|X W]) = V() = 0, the exact unconditional variance of the matching estimator equals the expected value of the conditional variance:

(3)

V(^) = E(V(^|X

W)) =

N0 N12

E[Ki2|Wi

=

0]

LEMMA 3.1--Exact Variance: Suppose that Assumptions 2.1, 2.2, and 3.1­3.4 hold. Then:
(i) The exact variance of the matching estimator is

V(^) = 1 + 3 (N1 - 1)(N0 + 8/3) N1 2 N1(N0 + 1)(N0 + 2)
(ii) As N  ,

N1 V(^ )



1

+

3 2



(iii)

 N

1(^

-

)

- d

N

0 1+ 3 2

See the Appendix for proofs.

3.3. The Bootstrap Variance
Now we analyze the properties of the bootstrap variance, V B I. As before, let Z = (X W Y) denote the original sample. Notice that

(4)

E[V B I] = E E[(^b - ^)2|Z] = E[(^b - ^)2]

is the expected bootstrap variance. Notice also that the expectation E[(^b - ^)2|Z] is taken over the bootstrap distribution (conditional on Z). The expectation E[(^b - ^)2] averages E[(^b - ^)2|Z] over the population distribution of
Z. Let Kb i be the number of times that unit i in the original sample is used as
a match in bootstrap sample b. For the DGP of Section 3.1,

(5)

^b

=



-

1 N1

N
Kb iYi
i=1

1544

A. ABADIE AND G. W. IMBENS

From equations (2) and (5) we obtain

(6)

E[(^b - ^)2] = E

1N

2

N1

(Kb i - Ki)Yi
i=1

=E

1 N12

N
(Kb i - Ki)2
i=1

=

N0 N12

E

(Kb i

- Ki)2|Wi

=0

The following lemma establishes the limit of N1E[V B I] under our DGP.

LEMMA 3.2 --Bootstrap Variance: Suppose that Assumptions 3.1­3.4 hold. Then, as N  ,

(7)

N1E[V

B

I]



1

+

3 2



5

exp(-1) - 2 exp(-2) 3(1 - exp(-1))

+

2

exp(-1)

Recall that the limit of the normalized variance of ^ is 1 + (3/2). For small values of  the limit of the expected bootstrap variance exceeds the limit variance by the third term in (7), 2 exp(-1) 0 74, or 74%. For large values of , the second term in (7) dominates and the ratio of the limit expected bootstrap and limit variance is equal to the factor in the second term of (7) multiplying (3/2). Since (5 exp(-1) - 2 exp(-2))/(3(1 - exp(-1))) 0 83, it follows that as  increases, the ratio of the limit expected bootstrap variance to the limit variance converges to 0.83, suggesting that in large samples the bootstrap variance can under- as well as overestimate the true variance.

3.4. Failure of the Bootstrap
So far, we have established the relationship between the limiting variance of the estimator and the limit of the average bootstrap variance. We end this section with a discussion of the implications of the previous two lemmas for the validity of the bootstrap. The bootstrap provides a valid estimator of the asymptotic variance of the simple matching estimator if

N1 E[(b - )2|Z] - V() - p 0

Lemmas 3.1 and 3.2 show that

N1V()

-

1

+

3 2



and

N1E[(b

-

)2]

-

1

+

3 2

5 

exp(-1) - 2 exp(-2) 3(1 - exp(-1))

+

2

exp(-1)

BOOTSTRAP FOR MATCHING ESTIMATORS

1545

Assume that the bootstrap provides a valid estimator of the asymptotic variance of the simple matching estimator. Then

N1E[(b

-

)2|Z]

- p

1

+

3 
2

Because N1E[(b - )2|Z]  0, it follows by the Portmanteau lemma (see, e.g., van der Vaart (1998, p. 6)) that, as N  ,

1+

3 2



lim E

N1E[(b

- )2|Z]

= lim N1E[(b - )2]

=

1

+

3 2



5

exp(-1) - 2 exp(-2) 3(1 - exp(-1))

+

2

exp(-1)

However, the algebraic inequality

1

+

3 2





1

+

35 
2

exp(-1) - 2 exp(-2) 3(1 - exp(-1))

+

2

exp(-1)

does not hold for large enough . As a result, the bootstrap does not provide a valid estimator of the asymptotic variance of the simple matching estimator.
The second version of the bootstrap provides a valid estimator of the asymptotic variance of the simple matching estimator if

N1 E (b - E[b|Z])2|Z - V() - p 0

Assume that the second version of the bootstrap provides a valid estimator of the asymptotic variance of the simple matching estimator. Then

N1E (b - E[b|Z])2|Z

- p 1 + 3  2

Notice that E[(b - E[b|Z])2|Z]  E[(b - )2|Z]. By the Portmanteau lemma, as N  ,

1+

3 
2



lim inf E

N1E

(b

- E[b|Z])2|Z

 lim E N1E[(b - )2|Z] = lim N1E[(b - )2]

=

1

+

3 2



5

exp(-1) - 2 exp(-2) 3(1 - exp(-1))

+

2

exp(-1)

Again, this inequality does not hold for large enough . As a result, the second version of the bootstrap does not provide a valid estimator of the asymptotic variance of the simple matching estimator.

1546

A. ABADIE AND G. W. IMBENS

Because the variance is an unbounded functional, inconsistency of the bootstrap variance does not necessarily imply inconsistency of the bootstrap estimator of the asymptotic distribution of matching estimators. However, using an argument similar to the one applied above, it is easy to see that the bootstrap distribution of N1(b - ) is not consistent, in general, for the asymptotic distribution of N1( - ). The reason is that if the bootstrap is consistent for the asymptotic distribution of N1( - ), then the limit inferior of the variance of N1(b - ) should not be smaller than 1 + (3/2), which we have shown happens for large enough .
As is apparent from equation (2), the matching estimator becomes linear after conditioning on X and W. The reason is that K1 KN are fixed once we condition on X and W. This implies that the wild bootstrap of Härdle and Mammen (1993) can be used to estimate the conditional distribution of matching estimators.8 The reason why the bootstrap fails to reproduce the unconditional distribution of ^ is that the bootstrap fails to reproduce the distribution of Ki, even in large samples. To gain some intuition about this, consider the DGP of Section 3.1. Equations (3), (4), and (6) imply that N1(E[V B I] - V(^))  0 if and only if E[(Kb i - Ki)2|Wi = 0] - E[Ki2|Wi = 0]  0. Consider the situation when  = N1/N0 is small. Then, because the number of control units is large relative to the number of treated units, most observations in the control group are used as a match no more than once, so Pr(Ki > 1|Wi = 0) is small. In a bootstrap sample, however, treated units can appear multiple times. Every time that a treated unit appears in the bootstrap sample, this unit is matched to the same control unit, creating instances in which Kb i - Ki > 1. This problem does not disappear by increasing the sample size. As a result, even in large samples, the bootstrap fails to reproduce the distribution of Ki and, in particular, it fails to reproduce E[Ki2|Wi = 0].

4. CONCLUSION
The results in this article have an immediate implication for empirical practice: bootstrap standard errors are not valid as the basis for inference with simple nearest-neighbor matching estimators with replacement and a fixed number of neighbors. In Abadie and Imbens (2006a), we proposed a valid estimator of the variance of matching estimators that is based on a normal approximation to the asymptotic distribution of these estimators. Simulation results in Abadie and Imbens (2006b) suggest that the analytic standard errors proposed in Abadie and Imbens (2006a) work well even in fairly small samples. Alternative inferential methods for matching estimators are the subsampling method of Politis and Romano (1994) and the M-out-of-N bootstrap of Bickel, Götze, and van Zwet (1997).

8We are grateful to a referee for suggesting this.

BOOTSTRAP FOR MATCHING ESTIMATORS

1547

In this article we consider only simple nearest-neighbor matching estimators with a fixed number of matches. Heckman, Ichimura, and Todd (1998) have proposed kernel-based matching methods for which the number of matches increases with the sample size. Because these estimators are asymptotically linear, we anticipate that the bootstrap provides valid inference. The same conjecture applies to other asymptotically linear estimators of average treatment effects, such as the propensity score weighting estimator proposed by Hirano, Imbens, and Ridder (2003). In addition, if Xi includes only discrete covariates with a finite number of possible values, then a simple matching estimator can be constructed to match each observation in the treatment group to all untreated observations with the same value of Xi. This matching estimator is just a weighted average of differences in means across groups defined by the values of the covariates. As a result, the standard bootstrap provides valid inference in this context.

APPENDIX
Before proving Lemma 3.1, we introduce some notation and preliminary results. Let X1 XN be a random sample from a continuous distribution. Let Mj be the index of the closest match for unit j. That is, if Wj = 1, then Mj is the unique index (ties happen with probability 0), with WMj = 0, such that Xj - XMj  Xj - Xi for all i such that Wi = 0. If Wj = 0, then Mj = 0. Let Ki be the number of times unit i is the closest match for a treated observation:
N
Ki = (1 - Wi) Wj1{Mj = i}
j=1

Following this definition, Ki is zero for treated units. Using this notation, we can write the estimator for the average treatment effect on the treated as

^ = 1 N1

N
(Wi - Ki)Yi
i=1

Also, let Pi be the probability that the closest match for a randomly chosen treated unit j is unit i, conditional on both the vector of treatment indicators W and on the vector of covariates for the control units X0:

Pi = Pr(Mj = i|Wj = 1 W X0)

For treated units, we define Pi = 0. The following lemma provides some properties of the order statistics of a
sample from the standard uniform distribution.

1548

A. ABADIE AND G. W. IMBENS

LEMMA A.1: Let X(1)  X(2)  · · ·  X(N) be the order statistics of a random sample of size N from a standard uniform distribution, U(0 1). Then, for 1  i  j  N,

E X(ri)(1 - X(j))s

= i[r](N - j + 1)[s] (N + 1)[r+s]

where for a positive integer a and a nonnegative integer b: a[b] = (a + b - 1)!/(a - 1)!. Moreover, for 1  i  N, X(i) has a Beta distribution with parameters (i N - i + 1); for 1  i  j  N, (X(j) - X(i)) has a Beta distribution with parameters (j - i N - (j - i) + 1).

The proof of this lemma and of other preliminary lemmas in this appendix are available in the working paper version of this article (Abadie and Imbens (2006b)).
Notice that the lemma implies the following results:

i E[X(i)] = N + 1 for 1  i  N

E[X(2i)]

=

(N

i(i + 1) + 1)(N +

2)

for

1iN

i(j + 1) E[X(i)X(j)] = (N + 1)(N + 2) for 1  i  j  N

First we investigate the first two moments of Ki, starting by studying the conditional distribution of Ki given X0 and W.

LEMMA A.2 --Conditional Distribution and Moments of Ki: Suppose that Assumptions 3.1­3.3 hold. Then the distribution of Ki conditional on Wi = 0, W, and X0 is binomial with parameters (N1 Pi):
Ki|Wi = 0 W X0  B(N1 Pi)

This implies the following conditional moments for Ki:
E[Ki|W X0] = (1 - Wi)N1Pi E[Ki2|W X0] = (1 - Wi)(N1Pi + N1(N1 - 1)Pi2)
To derive the marginal moments of Ki we need first to analyze the properties of the random variable Pi. Exchangeability of the units implies that the marginal expectation of Pi given N0, N1, and Wi = 0 is equal to 1/N0. To derive the second moment of Pi, it is helpful to express Pi in terms of the order statistics

BOOTSTRAP FOR MATCHING ESTIMATORS

1549

of the covariates for the control group. For control unit i, let (i) be the order of the covariate for the ith unit among control units:

N
(i) = (1 - Wj)1{Xj  Xi}
j=1

Furthermore, let X0(k) be the kth order statistic of the covariates among the control units, so that X0(1)  X0(2)  · · ·  X0(N0), and for control units, X0((i)) = Xi. Ignoring ties, which happen with probability zero, a treated unit with covariate value x will be matched to control unit i if

X0((i)-1) + X0((i)) < x < X0((i)+1) + X0((i))

2

2

if 1 < (i) < N0. If (i) = 1, then x will be matched to unit i if

x < X0(2) + X0(1) 2

and if (i) = N0, x will be matched to unit i if

X0(N0-1) + X0(N0) < x 2

To obtain Pi, we need to integrate the density of X conditional on W = 1, f1(x),

over these sets. With a uniform distribution for the covariates in the treatment

group (f1(x) = 1 for x  [0 1]), we obtain the following representation for Pi:

 

(X0(2)

+

X0(1))/2

if (i) = 1,

(A.1)

Pi

=



(X0((i)+1) - X0((i)-1))/2 1 - (X0(N0-1) + X0(N0))/2

if 1 < (i) < N0, if (i) = N0.

LEMMA A.3--Moments of Pi: Suppose that Assumptions 3.1­3.3 hold. Then: (i) The second moment of Pi conditional on Wi = 0 is

E[Pi2|Wi

=

0]

=

3N0 + 8 2N0(N0 + 1)(N0

+

2)

(ii) The Mth moment of Pi is bounded by

E[PiM |Wi = 0] 

1+M M N0 + 1

The proof of this lemma follows from equation (A.1) and Lemma A.1 (see Abadie and Imbens (2006b)).

1550

A. ABADIE AND G. W. IMBENS

PROOF OF LEMMA 3.1: First we prove (i). The first step is to calculate E[Ki2|Wi = 0]. Using Lemmas A.2 and A.3,
E[Ki2|Wi = 0] = N1E[Pi|Wi = 0] + N1(N1 - 1)E[Pi2|Wi = 0] = N1 + 3 N1(N1 - 1)(N0 + 8/3) N0 2 N0(N0 + 1)(N0 + 2)
Substituting this into (3), we get

V(^)

=

N0 N12

E[Ki2|Wi

=

0]

=

1 N1

+

3 2

(N1 - 1)(N0 + 8/3) N1(N0 + 1)(N0 + 2)

proving part (i). Next, consider part (ii). Multiply the exact variance of ^ by N1 and substitute
N1 = N0 to get

N1 V(^ )

=

1

+

3 2

(N0 - 1)(N0 + 8/3) (N0 + 1)(N0 + 2)

Then take the limit as N0   to get

lim
N 

N1 V(^ )

=

1

+

3 
2

Finally, consider part (iii). Let S(r j) be a Stirling number of the second kind. For any nonnegative integer M, the Mth moment of Ki given W and X0 is (Johnson, Kotz, and Kemp (1993))

E[KiM |X0

M
Wi = 0] =
j=0

S(M j)N1!Pij (N1 - j)!

Therefore, applying Lemma A.3(ii), we obtain that the moments of Ki are uniformly bounded:

E[KiM |Wi = 0] =

M j=0

S(M (N1

j)N1! - j)!

E[Pij

|Wi

=

0]

 M S(M j)N1! 1 + M j j=0 (N1 - j)! N0 + 1

M
 S(M j)j(1 + M)j
j=0

BOOTSTRAP FOR MATCHING ESTIMATORS

1551

Notice that

E

1 N1

N i=1

Ki2

=

N0 N1

E[Ki2|Wi

=

0]



1

+

3 
2

V

1 N1

N
Ki2
i=1



N0 N12

V(Ki2|Wi

=

0)



0

because cov(Ki2 Kj2|Wi = Wj = 0 i = j)  0 (see Joag-Dev and Proschan (1983)). Therefore,

1 N1

N

Ki2

p

1

+

3 2

i=1

Finally, we write

^ -  =

1 N1

N i=1

i

where i = -KiYi. Conditional on X and W, the i are independent, and the distribution of i is degenerate at zero for Wi = 1 and normal N (0 Ki2) for Wi = 0. Hence, for any c  R,

Pr

1 N1

N
Ki2
i=1

-1/2

N1(^ - )  c X W

=

(c)

where (·) is the cumulative distribution function of a standard normal variable. Integrating over the distribution of X and W yields

Pr

1 N1

N
Ki2
i=1

-1/2

N1(^ - )  c

=

(c)

Now, Slustky's theorem implies (iii).

Q.E.D.

Next we introduce some additional notation. Let Rb i be the number of times unit i is in the bootstrap sample. In addition, let Db i be an indicator for inclusion of unit i in the bootstrap sample, so that Db i = 1{Rb i > 0}. Let Nb 0 = iN=1(1 - Wi)Db i be the number of distinct control units in the bootstrap sample. Finally, define the binary variable Bi(x) for i = 1 N to be the in-
dicator for the event that in the bootstrap sample a treated unit with covariate
value x would be matched to unit i. That is, for this indicator to be equal to 1,

1552

A. ABADIE AND G. W. IMBENS

the following three conditions need to be satisfied: (i) unit i is a control unit, (ii) unit i is in the bootstrap sample, and (iii) the distance between Xi and x is less than or equal to the distance between x and any other control unit in the bootstrap sample. Formally,

1 Bi(x) =
0

if |x - Xi| = min |x - Xk| and Db i = 1 Wi = 0, k:Wk=0 Db k=1
otherwise.

For the N units in the original sample, let Kb i be the number of times unit i is used as a match in the bootstrap sample:

N
Kb i = WjBi(Xj)Rb j
j=1

Equation (6) implies

N1E[V

B

I]

=

1 

E[(Kb

i

-

Ki)2|Wi

=

0]

The first step in deriving this expectation is to establish some properties of Db i, Rb i, Nb 0, and Bi(x).

LEMMA A.4--Properties of Db i, Rb i, Nb 0, and Bi(x): Suppose that Assump-

tions 3.1­3.3 hold. Then, for w  {0 1} and n  {1 N0}:

(i) Rb i|Wi = w Z  B(Nw 1/Nw);

(ii) Db i|Wi = w Z  B(1 1 - (1 - 1/Nw)Nw );

(iii)

Pr(Nb 0 = n) =

N0 N0 -n

(n!/N0N0 )S(N0

n);

(iv) Pr(Bi(Xj) = 1|Wj = 1 Wi = 0 Db i = 1 Nb 0) = 1/Nb 0;

(v) For l = j,

Pr Bi(Xl)Bi(Xj) = 1|Wj = Wl = 1 Wi = 0 Db i = 1 Nb 0

=

3Nb 0 + 8

;

2Nb 0(Nb 0 + 1)(Nb 0 + 2)

(vi) E[Nb 0/N0] = 1 - (1 - 1/N0)N0  1 - exp(-1); (vii) (1/N0)V(Nb 0) = (N0 - 1)(1 - 2/N0)N0 + (1 - 1/N0)N0 - N0(1 - 1/N0)2N0  exp(-1)(1 - 2 exp(-1))

Next, we prove a general result for the bootstrap. Consider a sample of size

N, indexed by i = 1 N. Let Db i indicate whether observation i is in boot-

strap sample b. Let Nb =

N i=1

Db

i

be

the

number

of

distinct

observations

in

bootstrap sample b.

BOOTSTRAP FOR MATCHING ESTIMATORS

1553

LEMMA A.5--Bootstrap: For all m  0,

E N - Nb m  exp(-m) N

and

Nm

E



1

m

Nb

1 - exp(-1)

LEMMA A.6--Approximate Bootstrap K Moments: Suppose that Assump-

tions 3.1­3.3 hold. Then:

(i)

E[Kb2 i|Wi = 0]  2 +

3 2

(2/(1

-

exp(-1)));

(ii)

E[Kb iKi|Wi = 0]  (1 - exp(-1))( +

3 2

2)

+

2

exp(-1)

PROOF: Here we prove (i). The proof of part (ii) is similar in spirit, but much longer (see Abadie and Imbens (2006b)). Notice that for i j l, such that Wi = 0 and Wj = Wl = 1,

(Rb j Rb l)  Db i Bi(Xj) Bi(Xl)

Notice also that {Rb j : Wj = 1} are exchangeable with

Rb j = N1
Wj =1

Therefore, applying Lemma A.4(i), for Wj = Wl = 1,

cov(Rb j

Rb

l

)

=

-

V(Rb (N1 -

j) 1)

=

- 1 - 1/N1 (N1 - 1)

-

0

As a result,

E[Rb jRb l|Db i = 1 Bi(Xj) = Bi(Xl) = 1 Wi = 0 Wj = Wl = 1 j = l] - E[Rb j|Db i = 1 Bi(Xj) = Bi(Xl) = 1 Wi = 0 Wj = Wl = 1 j = l] 2 - 0

By Lemma A.4(i),

E[Rb j|Db i = 1 Bi(Xj) = Bi(Xl) = 1 Wi = 0 Wj = Wl = 1 j = l] = 1

Therefore,

E[Rb jRb l|Db i = 1 Bi(Xj) = Bi(Xl) = 1 Wi = 0 Wj = Wl = 1 j = l] - 1

1554

A. ABADIE AND G. W. IMBENS

In addition,

E[R2b j|Db i = 1 Bi(Xj) = 1 Wj = 1 Wi = 0] = N1(1/N1) + N1(N1 - 1)(1/N12) - 2
Notice that

Pr(Db i = 1|Wi = 0 Wj = Wl = 1 j = l Nb 0)

= Pr(Db i = 1|Wi = 0

Nb 0) =

Nb 0 N0

Using Bayes' rule,

Pr(Nb 0 = n|Db i = 1 Wi = 0 Wj = Wl = 1 j = l) = Pr(Nb 0 = n|Db i = 1 Wi = 0) = Pr(Db i = 1|Wi = 0 Nb 0 = n) Pr(Nb 0 = n) Pr(Db i = 1|Wi = 0) = n/N0 Pr(Nb 0 = n) 1 - (1 - 1/N0)N0
Therefore,

N0 Pr(Bi(Xj) = 1|Db i = 1 Wi = 0 Wj = 1)

N0
= N0 Pr(Bi(Xj) = 1|Db i = 1 Wi = 0 Wj = 1 Nb 0 = n)
n=1

× Pr(Nb 0 = n|Db i = 1 Wi = 0 Wj = 1)

= N0

N0

1 n

n=1

n N0

Pr(Nb 0 = n) =

1

1 - (1 - 1/N0)N0 1 - (1 - 1/N0)N0

-

1

1 - exp(-1)

In addition,

N02 Pr Bi(Xj)Bi(Xl)|Db i = 1 Wi = 0 Wj = Wl = 1 j = l Nb 0

=3

N02(Nb 0 + 8/3)

- p 3

1

2

2 Nb 0(Nb 0 + 1)(Nb 0 + 2)

2 1 - exp(-1)

BOOTSTRAP FOR MATCHING ESTIMATORS

1555

Therefore,

N0
N02 Pr(Bi(Xj)Bi(Xl)|Db i = 1 Wi = 0 Wj = Wl = 1 j = l Nb 0) 2
n=1

× Pr(Nb 0 = n|Db i = 1 Wi = 0 Wj = Wl = 1 j = l)

N0
=

3 N02(n + 8/3)

2 n/N0 Pr(Nb 0 = n)

2 n(n + 1)(n + 2)
n=1

1 - (1 - 1/N0)N0

9 4

1 1 - exp(-1)

N0

N04(n

+ n6

8/3)2

Pr(Nb

0

=

n)

n=1

Notice that

N0

N04(n

+ n6

8/3)2

Pr(Nb

0

=

n)

n=1

 1 + 16 + 64 39

N0

N0 n

4
Pr(Nb 0 = n)

n=1

which is bounded away from infinity (this is shown in the proof of Lemma A.5). Convergence in probability of a random variable along with boundedness of its second moment implies convergence of the first moment (see, e.g., van der Vaart (1998)). As a result,

N02 Pr Bi(Xj)Bi(Xl)|Db i = 1 Wi = 0 Wj = Wl = 1 j = l

- 3

1

2

2 1 - exp(-1)

Then, using these preliminary results, we obtain

E[Kb2 i|Wi = 0]

NN

=E

WjWlBi(Xj)Bi(Xl)Rb jRb l Wi = 0

j=1 l=1

N

=E

WjBi(Xj)R2b j Wi = 0

j=1

N

+E

WjWlBi(Xj)Bi(Xl)Rb jRb l Wi = 0

j=1 l=j

1556

A. ABADIE AND G. W. IMBENS

= N1E[R2b j|Db i = 1 Bi(Xj) = 1 Wj = 1 Wi = 0]

× Pr(Bi(Xj) = 1|Db i = 1 Wj = 1 Wi = 0)

× Pr(Db i = 1|Wj = 1 Wi = 0)

+ N1(N1 - 1)E[Rb jRb l|Db i = 1 Bi(Xj) = Bi(Xl) = 1

Wj = Wl = 1 j = l Wi = 0]

× Pr(Bi(Xj)Bi(Xl) = 1|Db i = 1 Wj = Wl = 1 j = l Wi = 0)

× Pr(Db i = 1|Wj = Wl = 1 j = l Wi = 0)

3

2

- 2 +

2 (1 - exp(-1))

Q.E.D.

PROOF OF LEMMA 3.2: From preliminary results,

N1E[V B I]

=

1 

(E[Kb2

i|Wi

=

0]

-

2E[Kb

iKi|Wi

=

0]

+

E[Ki2|Wi

=

0])

1

3

2

 2 +



2 (1 - exp(-1))

- 2(1 - exp(-1))  + 3 2 + exp(-1) 2 +  + 3 2

2 1 - exp(-1)

2

=

2(1

-

3 exp(-1))

-

3(1

-

exp(-1))

-

2

exp(-1)

+

3 2

+ 2 - 2 + 2 exp(-1) + 1

=

1

+

35 
2

exp(-1) - 2 exp(-2) 3(1 - exp(-1))

+

2

exp(-1)

Q.E.D.

REFERENCES
ABADIE, A., AND G. IMBENS (2006a): "Large Sample Properties of Matching Estimators for Average Treatment Effects," Econometrica, 74, 235­267. [1538,1540,1541,1546] (2006b): "On the Failure of the Bootstrap for Matching Estimators," Working Paper, available at http://www.ksg.harvard.edu/fs/aabadie. [1546,1548,1549,1553] (2007): "Bias-Corrected Matching Estimators for Average Treatment Effects," Unpublished Manuscript, John F. Kennedy School of Government, Harvard University. [1540]
ARTHREYA, K. (1987): "Bootstrap of the Mean in the Infinite Variance Case," Annals of Statistics, 15, 724­731. [1538]
BERAN, R. (1984): "Bootstrap Methods in Statistics," Jahresberichte des Deutschen Mathematischen Vereins, 86, 14­30. [1538]
BICKEL, P., AND D. FREEDMAN (1981): "Some Asymptotic Theory for the Bootstrap," Annals of Statistics, 9, 1196­1217. [1537]

BOOTSTRAP FOR MATCHING ESTIMATORS

1557

BICKEL, P., F. GÖTZE, AND W. VAN ZWET (1997): "Resampling Fewer Than n Observations: Gains, Losses and Remedies for Losses," Statistica Sinica, 7, 1­31. [1538,1546]
DEHEJIA, R., AND S. WAHBA (1999): "Causal Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs," Journal of the American Statistical Association, 94, 1053­1062. [1537]
HALL, P. (1992): The Bootstrap and Edgeworth Expansions. New York: Springer Verlag. [1538] HÄRDLE, W., AND E. MAMMEN (1993): "Comparing Nonparametric versus Parametric Regres-
sion Fits," Annals of Statistics, 21, 1926­1947. [1546] HECKMAN, J., H. ICHIMURA, AND P. TODD (1998): "Matching as an Econometric Evaluation
Estimator," Review of Economic Studies, 65, 261­294. [1538,1547] HIRANO, K., G. IMBENS, AND G. RIDDER (2003): "Efficient Estimation of Average Treatment
Effects Using the Estimated Propensity Score," Econometrica, 71, 1161­1189. [1547] HOROWITZ, J. (2003): "The Bootstrap," in Handbook of Econometrics, Vol. 5, ed. by J. Heckman
and E. Leamer. New York: Elsevier Science. [1538] IMBENS, G. (2004): "Nonparametric Estimation of Average Treatment Effects Under Exogene-
ity," Review of Economics and Statistics, 86, 4­29. [1537,1538] JOAG-DEV, AND F. PROSCHAN (1983): "Negative Association of Random Variables, With Appli-
cations," The Annals of Statistics, 11, 286­295. [1551] JOHNSON, N., S. KOTZ, AND A. KEMP (1993): Univariate Discrete Distributions (Second Ed.). New
York: Wiley. [1550] POLITIS, N., AND J. ROMANO (1994): "Large Sample Confidence Regions Based on Subsamples
Under Minimal Assumptions," The Annals of Statistics, 22, 2031­2050. [1538,1546] POLITIS, N., J. ROMANO, AND M. WOLF (1999): Subsampling. New York: Springer Verlag. [1538] ROSENBAUM, P. (2001): Observational Studies (Second Ed.). New York: Springer Verlag. [1537,
1538] ROSENBAUM, P., AND D. RUBIN (1983): "The Central Role of the Propensity Score in Observa-
tional Studies for Causal Effects," Biometrika, 70, 41­55. [1538] VAN DER VAART, A. (1998): Asymptotic Statistics. New York: Cambridge University Press. [1545,
1555]

John F. Kennedy School of Government, Harvard University, 79 John F. Kennedy Street, Cambridge, MA 02138, U.S.A. and NBER; alberto_abadie@ harvard.edu, http://www.ksg.harvard.edu/fs/aabadie/
and Dept. of Economics, Harvard University, M-24 Litauer Center, 1805 Cambridge Street, Cambridge, MA 02138, U.S.A. and NBER; imbens@harvard.edu, http://www.economics.harvard.edu/faculty/imbens/imbens.html.
Manuscript received May, 2006; final revision received May, 2008.

