{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Session 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:**\n",
    "[Helge Liebert](https://hliebert.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text analysis**: Kiva loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "library(tm)\n",
    "library(data.table)\n",
    "library(ggplot2)\n",
    "library(tidytext)\n",
    "library(dplyr)\n",
    "library(topicmodels)\n",
    "library(wordcloud)\n",
    "library(SentimentAnalysis)\n",
    "library(naivebayes)\n",
    "library(slam)\n",
    "library(glmnet)\n",
    "library(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple helper function to view first copora elements, only for illustration in lecture\n",
    "chead <- function(c) lapply(c[1:2], as.character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a corpus and applying transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial relies on the Kiva data from the last lecture. The text analyses are based on the loan description and to a limited extent the loan purpose statement as well. The data is based on a csv database dump provided on their site. To ease computation we just use a limited sample of 10,000 observations. Using the csv file from the Kiva Homepage and the file `prep-kiva.r`, you can clean the data yourself and use a larger sample. The full sample is close to one million observations. (You might need to delete a few nested parentheses in the loan description using a text editor which lead to errors reading the csv file.) If you look at the script, you can also see that I spent some time pre-processing the data, filtering HTML tags and similar things, to get mostly clean loan descriptions. This type of pre-processing is common, but also very application specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are using Windows, or any other OS not using UTF-8 encoding as default, setting the encoding when reading data files is good practice. When working with text data, take care to ensure you are using the correct encoding and that transformations between files and encodings do not lead to broken characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "loans <- fread(\"Data/kiva-tiny.csv\", encoding = \"UTF-8\")\n",
    "names(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to do is set up a corpus. We will focus on the loan description. For all transformations in the remainder of this tutorial, we are going to print the first two loans' descriptions for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up corpus\n",
    "setnames(loans, \"loanid\", \"doc_id\")\n",
    "setnames(loans, \"description\", \"text\")\n",
    "corp <- Corpus(DataframeSource(loans))\n",
    "\n",
    "## Inspect it\n",
    "corp\n",
    "lapply(corp[1:2], as.character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the main transformations available in the `tm` library, but any other customized transformation can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main corpus transformations, passed via tm_map()\n",
    "## Other transformations have to be wrapped in content_transformer()\n",
    "getTransformations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the `base` string function `tolower()` to transform all strings to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All chars to lower case\n",
    "corp <- tm_map(corp, content_transformer(tolower))\n",
    "chead(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all punctuation as punctuation is unlikely to carry special meaning in the context of loans and we want to simplify the text input to get token counts. We need to set the unicode option to true to rid of all punctuation elements (eg. the quotation marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove punctuation\n",
    "corp <- tm_map(corp, removePunctuation)\n",
    "chead(corp)\n",
    "## corp <- tm_map(corp, removePunctuation, ucp = TRUE)\n",
    "## chead(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove all numbers. We observe the loan amount and the repayment schedule in other variables, so we can get rid of numbers. Extracting the meaning of numbers within their context is difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove numbers\n",
    "corp <- tm_map(corp, removeNumbers)\n",
    "chead(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any other transformation - like substituting specific patterns based on regular expressions - can be passed to `tm_map()` using a user-defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For specific transformations, you could also pass a lambda function to remove patterns based on a regex\n",
    "\n",
    "## Example:\n",
    "## toSpace <- content_transformer(function (x , pattern) gsub(pattern, \" \", x))\n",
    "## corp <- tm_map(corp, toSpace, \"patternhere\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a frequency plot of the token counts, there is still plenty of filtering to do to get meaningful token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the most frequent words in our text and see whether we should get rid of some\n",
    "frequent_terms <- qdap::freq_terms(corp, 30)\n",
    "plot(frequent_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove stopwords and other generic words which do not carry special meaning in our context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More invasive changes: remove generic and custom stopwords\n",
    "corp <- tm_map(corp, removeWords, stopwords('english'))\n",
    "chead(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And a few more words we filter for lack of being informative, this could be extended\n",
    "corp <- tm_map(corp, removeWords, \"loan\")\n",
    "corp <- tm_map(corp, removeWords, \"kiva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are a lot of names in the data, these are not really informative\n",
    "## We apply a dictionary to get rid of some of them\n",
    "## Truncation because of regex limit\n",
    "corp <- tm_map(corp, removeWords, common_names[1:floor(length(common_names)/2)])\n",
    "corp <- tm_map(corp, removeWords, common_names[floor(length(common_names)/2):length(common_names)])\n",
    "corp <- tm_map(corp, removeWords, freq_first_names[1:floor(nrow(freq_first_names)/2), Name])\n",
    "corp <- tm_map(corp, removeWords, freq_first_names[floor(nrow(freq_first_names)/2):nrow(freq_first_names), Name])\n",
    "## corp <- tm_map(corp, removeWords, freq_last_names) # needs to be truncated as well, even longer\n",
    "chead(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stem the document here. For illustration purposes I refrain from it here, but in real applications you might want to do this. Stemmers do not work equally well for all languages, depending on your application, there may be added value to grouping and transforming tokens further yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stem document\n",
    "## corp <- tm_map(corp, stemDocument, language = 'english')\n",
    "## chead(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip all extra whitespace (this is without consequences for tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Strip extra whitespace\n",
    "corp <- tm_map(corp, stripWhitespace)\n",
    "chead(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a document-term matrix and restricting the feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the corpus to a document-term matrix. We use simple term-frequency weighting, i.e. the simple token counts. This is the default. You can also choose term frequency-inverse document frequency (tfidf) weighting at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a document-term or term-document matrix\n",
    "## Default is term-frequency weighting (document length normalized count)\n",
    "## TF-IDF weighting also possible\n",
    "\n",
    "## dtm <- TermDocumentMatrix(corp)\n",
    "dtm <- DocumentTermMatrix(corp)\n",
    "\n",
    "## Inspect the document-term matrix\n",
    "inspect(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the tokenization, we can inspect the most popular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect most popular words\n",
    "findFreqTerms(dtm, lowfreq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document-word vectors allow us to inspect the correlation between words as we would between variables in other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect associations\n",
    "findAssocs(dtm, 'hard', 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the matrix is very wide and sparse, we are going to remove terms to arrive at a tractable representation. We can filter words simply by removing words that are very rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove sparse terms, prevents cluster node from choking and saves time\n",
    "## may also improve tractability\n",
    "\n",
    "## Tweak the sparse parameter to influence # of words\n",
    "dtms <- removeSparseTerms(dtm, sparse=0.90)\n",
    "dim(dtms)\n",
    "dtms <- dtms[row_sums(dtms) > 0, ]\n",
    "dim(dtms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter by tf-idf, only keeping words which occur frequently in some documents but not in others, helping us to keep those words that disambiguate loans. We compute the average tf-idf score for each token, then filter by that. I went back-and-forth a bit tweaking the threshold value for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively, filter words by mean tf-idf\n",
    "\n",
    "## Calculate average term-specific tf-idf weights as\n",
    "## mean(word count/document length) * log(ndocs/ndocs containing word)\n",
    "termtfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) *\n",
    "             log(nDocs(dtm)/col_sums(dtm > 0))\n",
    "summary(termtfidf)\n",
    "\n",
    "## Only include terms with at least median tf-idf score\n",
    "dtmw <- dtm[, (termtfidf >= 0.15)]\n",
    "dim(dtmw)\n",
    "## And documents within which these terms occur - this may induce selection\n",
    "dtmw <- dtmw[row_sums(dtmw) > 0, ]\n",
    "dim(dtmw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Much less frequent terms now\n",
    "findFreqTerms(dtmw, lowfreq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations of word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just very simple visualizations of word frequencies. First the unfiltered but transformed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple visualization\n",
    "wordcloud(corp, max.words = 100, random.order = FALSE,\n",
    "          colors = brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the term-frequency filtered document-term matrix. Obviously this is very similar to the plot above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counts from dtms\n",
    "counts <- sort(colSums(as.matrix(dtms)), decreasing = TRUE)\n",
    "counts <- data.frame(word = names(counts), freq = counts)\n",
    "wordcloud(words = counts$word, freq = counts$freq,\n",
    "          max.words = 100, random.order = FALSE,\n",
    "          colors = brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the frequency plot of the tfidf-filtered document-term matrix looks different. There are a lot more terms that disambiguate professions and investment goods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counts from dtmw\n",
    "counts <- sort(colSums(as.matrix(dtmw)), decreasing = TRUE)\n",
    "counts <- data.frame(word = names(counts), freq = counts)\n",
    "wordcloud(words = counts$word, freq = counts$freq,\n",
    "          max.words = 100, random.order = FALSE,\n",
    "          colors = brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary methods: Inferring sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a fixed mapping of terms to infer a sentiment score, and then convert it to discrete sentiment categories. Unsuprisingly, most loan descriptions are phrased to convey a positive message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary method: Sentiment analysis using dictionaries\n",
    "sentiment <- analyzeSentiment(dtms, language = \"english\")\n",
    "sentiment <- convertToDirection(sentiment$SentimentGI)\n",
    "\n",
    "## Potentially add back to original data for further analysis\n",
    "## loans$sentiment <- sentiment\n",
    "\n",
    "## look at sentiment distribution\n",
    "table(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised generative model: Topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to train an unsupervised topic model on the term-frequency filtered document term matrix. You will find that it is hard to find distinct topics, both due to the term filtering, and the fact that most loans are handed out by partner organizations who use a standard questionnaire to get basic information which is then translated to an english description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unsupervised method: Topic model\n",
    "lda <- LDA(dtms, k = 5, control = list(seed = 100))\n",
    "## lda <- LDA(dtmw, k = 5, control = list(seed = 100))\n",
    "\n",
    "## Most likely topic for each document, could merge this to original data\n",
    "## topic <- topics(lda, 1)\n",
    "\n",
    "## Five most frequent terms for each topic\n",
    "terms(lda, 10)\n",
    "\n",
    "## Plot most frequent terms and associated probabilities by topic\n",
    "tpm <- tidy(lda, matrix = \"beta\")\n",
    "\n",
    "topterms <-\n",
    "    tpm %>%\n",
    "    group_by(topic) %>%\n",
    "    top_n(10, beta) %>%\n",
    "    ungroup() %>%\n",
    "    arrange(topic, -beta)\n",
    "\n",
    "topterms %>%\n",
    "    mutate(term = reorder(term, beta)) %>%\n",
    "    ggplot(aes(term, beta, fill = factor(topic))) +\n",
    "    geom_col(show.legend = FALSE) +\n",
    "    facet_wrap(~ topic, scales = \"free\") +\n",
    "    coord_flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let us use the `loan use` statement, filtered by tfidf. These topics already look more distinct. You could tweak the filter and the number of topics further to arrive at a more meaningful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not working well due to standardized templates\n",
    "## let us try to use the `loanuse' statement text for the generative topic model \n",
    "\n",
    "# new data\n",
    "loanuse <- loans[, .(doc_id, loanuse)]\n",
    "setnames(loanuse, \"loanuse\", \"text\")\n",
    "\n",
    "# new dtm, this time do most of the transformations in one step\n",
    "dtmuse <- DocumentTermMatrix(Corpus(DataframeSource(loanuse)),\n",
    "                             control = list(weighting = weightTf,\n",
    "                                            language = \"english\",\n",
    "                                            tolower = TRUE,\n",
    "                                            removePunctuation = TRUE,\n",
    "                                            removeNumbers = TRUE,\n",
    "                                            stopwords = TRUE,\n",
    "                                            stemming = FALSE,\n",
    "                                            wordLengths = c(3, Inf)))\n",
    "inspect(dtmuse)\n",
    "\n",
    "# Recalculate weights\n",
    "termtfidf <- tapply(dtmuse$v/row_sums(dtmuse)[dtmuse$i], dtmuse$j, mean) *\n",
    "    log2(nDocs(dtmuse)/col_sums(dtmuse > 0))\n",
    "summary(termtfidf)\n",
    "\n",
    "## Filter by tf-idf\n",
    "## dim(dtmuse)\n",
    "dtmuse <- dtmuse[, (termtfidf >= 1.70)]\n",
    "dtmuse <- dtmuse[row_sums(dtmuse) > 0, ]\n",
    "## dim(dtmuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unsupervised method: Topic model, this time for loanuse statement\n",
    "lda <- LDA(dtmuse, k = 3, control = list(seed = 100))\n",
    "## str(lda)\n",
    "\n",
    "## Most likely topic for each document, could merge this to original data\n",
    "topic <- topics(lda, 1)\n",
    "## Five most frequent terms for each topic\n",
    "terms(lda, 10)\n",
    "\n",
    "## Plot most frequent terms and associated probabilities by topic\n",
    "tpm <- tidy(lda, matrix = \"beta\")\n",
    "\n",
    "topterms <-\n",
    "    tpm %>%\n",
    "    group_by(topic) %>%\n",
    "    top_n(10, beta) %>%\n",
    "    ungroup() %>%\n",
    "    arrange(topic, -beta)\n",
    "\n",
    "topterms %>%\n",
    "    mutate(term = reorder(term, beta)) %>%\n",
    "    ggplot(aes(term, beta, fill = factor(topic))) +\n",
    "    geom_col(show.legend = FALSE) +\n",
    "    facet_wrap(~ topic, scales = \"free\") +\n",
    "    coord_flip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at unique terms not appearing in other topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqterms <- terms(lda, 40)\n",
    "duplicates <- c(freqterms)[duplicated(c(freqterms))]\n",
    "distinctterms <- lapply(as.list(as.data.frame(freqterms)), function(x) x[!(x %in% duplicates)])\n",
    "distinctterms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised methods: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells transform and prep the data to be used as inputs for supervised methods. We split the data into a test and a training sample. The outcome we try to predict is whether the loan is obtained for a business proposition in the agricultural sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supervised methods: Prep data\n",
    "## Convert the sparse term-document matrix to a standard data frame\n",
    "bag <- as.data.frame(as.matrix(dtms))\n",
    "dim(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert token counts to simple binary indicators\n",
    "bag <- as.data.frame(sapply(bag, function(x) as.numeric(x > 0)))\n",
    "bag$doc_id <- rownames(as.matrix(dtms))\n",
    "\n",
    "## Add outcomes from the original data: Predict agricultural sector\n",
    "loans$agsector <- as.numeric(loans$sectorname==\"Agriculture\")\n",
    "bag <- merge(bag, loans[, .(agsector, loanamount, doc_id)], by = \"doc_id\")\n",
    "                            \n",
    "# How many people want a loan in the agricultural sector?                            \n",
    "table(bag$agsector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partition data in test and training sample\n",
    "set.seed(100)\n",
    "testids <- sample(floor(nrow(bag)/3))\n",
    "xtrain <- as.matrix(bag[-testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\n",
    "ytrain <- as.factor(bag[-testids,  \"agsector\"])\n",
    "xtest  <- as.matrix(bag[ testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\n",
    "ytest  <- as.factor(bag[ testids,  \"agsector\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised generative model: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a simple model relying on a conditional independence assumption of the token counts. It often performs acceptable. In this case it does not perform very well, possibly because we filtered the input token data to aggressively. Among other things, there is no need to remove stopwords here, and it may not improve performance. You can feed the unfiltered data or try different transformations and see whether this improves matters. However, Naive Bayes may also be not well suited for this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supervised generative model: Naive Bayes\n",
    "nbclassifier <- naive_bayes(xtrain, ytrain, laplace = 1)\n",
    "nbpred <- predict(nbclassifier, xtest)\n",
    "summary(nbpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance statistics: Classification rate\n",
    "round(1-mean(as.numeric(nbpred != ytest)), 2)\n",
    "\n",
    "## Performance statistics: Confusion matrix (\n",
    "## table(nbpred, ytest)\n",
    "caret::confusionMatrix(nbpred, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supervised text regression: L1 penalized logistic regression\n",
    "l1classifier <- cv.glmnet(xtrain, ytrain, alpha = 1, family = \"binomial\")\n",
    "l1pred <- as.factor(predict(l1classifier, xtest, s = \"lambda.min\", type = \"class\"))\n",
    "summary(l1pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance statistics: Classification rate\n",
    "round(1-mean(as.numeric(l1pred != ytest)), 2)\n",
    "\n",
    "## Performance statistics: Confusion matrix\n",
    "caret::confusionMatrix(l1pred, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised text regression: L<sub>1</sub> penalized logistic classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trains a logistic lasso estimator, weighting the penalty factor for each input token by the token's standard deviation. Looking at the misclassification rate and the confusion matrix, the model performs better than naive bayes in predicting the agricultural sector. However, looking at precision and recall, the model does poor in getting the true condition outcomes right, leading to a large number of false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## L1 logistic classifier using rare feature upweighting\n",
    "# l1classifier <- cv.glmnet(xtrain, ytrain, alpha = 1, family = \"binomial\")\n",
    "## L1 logistic classifier using rare feature upweighting\n",
    "sdweights <- apply(xtrain, 2, sd)\n",
    "l1classifier <- cv.glmnet(xtrain, ytrain, alpha = 1, family = \"binomial\",\n",
    "                          standardize = FALSE, penalty.factor  = sdweights)\n",
    "l1pred <- as.factor(predict(l1classifier, xtest, s = \"lambda.min\", type = \"class\",\n",
    "                            penalty.factor  = sdweights))\n",
    "summary(l1pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance statistics: Classification rate\n",
    "round(1-mean(as.numeric(l1pred != ytest)), 2)\n",
    "## Performance statistics: Confusion matrix\n",
    "caret::confusionMatrix(l1pred, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks and additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How would you go about improving performce for the classifiers?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Addendum: Regression example: L<sub>1</sub> penalized linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Further example: Predict Loan Amount\n",
    "## Supervised text regression: L1 penalized linear regression\n",
    "\n",
    "## Rebuild outcome vectors\n",
    "#ytrain <- as.matrix(bag[-testids,  \"loanamount\"])\n",
    "#ytest  <- as.matrix(bag[ testids,  \"loanamount\"])\n",
    "\n",
    "## Estimate and predict\n",
    "#l1predictor <- cv.glmnet(xtrain, ytrain, alpha = 1, family = \"gaussian\")\n",
    "#l1pred <- predict(l1predictor, xtest, s = \"lambda.min\", type = \"response\")\n",
    "\n",
    "## RMSE\n",
    "#round(sqrt(mean((l1pred - ytest)^2)), 2)\n",
    "#caret::postResample(l1pred, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
